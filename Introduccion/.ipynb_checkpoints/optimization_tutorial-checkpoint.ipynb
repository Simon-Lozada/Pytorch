{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing Model Parameters\n",
    "===========================\n",
    "Ahora que tenemos un modelo y datos, es hora de entrenar, validar y probar nuestro modelo optimizando sus parámetros en\n",
    " nuestros datos.  El entrenamiento de un modelo es un proceso iterativo;  en cada iteración (llamada *época*), el modelo hace una conjetura sobre la salida, calcula\n",
    " el error en su conjetura (*pérdida*), recoge las derivadas del error con respecto a sus parámetros (como vimos en\n",
    " la `sección anterior <autograd_tutorial.html>`_), y **optimiza** estos parámetros mediante el descenso de gradiente.  Para una mayor\n",
    " tutorial detallado de este proceso, mira este video sobre `backpropagation from 3Blue1Brown <https://www.youtube.com/watch?v=tIeHLnjs5U8>`__.__.\n",
    "\n",
    "Código de requisito previo\n",
    " -----------------\n",
    "Cargamos el código de las secciones anteriores en `Datasets & DataLoaders <data_tutorial.html>`_\n",
    "y `Crear modelo <buildmodel_tutorial.html>`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparámetros\n",
    "-----------------\n",
    "\n",
    "Los hiperparámetros son parámetros ajustables que le permiten controlar el proceso de optimización del modelo.\n",
    "Diferentes valores de hiperparámetros pueden afectar el entrenamiento del modelo y las tasas de convergencia\n",
    "(`lea más <https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>`__ sobre el ajuste de hiperparámetros)\n",
    "\n",
    "Definimos los siguientes hiperparámetros para el entrenamiento:\n",
    "  - **Número de épocas**: el número de veces para iterar sobre el conjunto de datos\n",
    "  - **Tamaño del lote**: la cantidad de muestras de datos propagadas a través de la red antes de que se actualicen los parámetros\n",
    "  - **Tasa de aprendizaje**: cuánto actualizar los parámetros de los modelos en cada lote/época. Los valores más pequeños producen una velocidad de aprendizaje lenta, mientras que los valores grandes pueden provocar un comportamiento impredecible durante el entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Loop\n",
    "-----------------\n",
    "\n",
    "Una vez que configuramos nuestros hiperparámetros, podemos entrenar y optimizar nuestro modelo con un ciclo de optimización.  Cada\n",
    " la iteración del bucle de optimización se denomina **época**.\n",
    "\n",
    " Cada época consta de dos partes principales:\n",
    "  - **The Train Loop**: itere sobre el conjunto de datos de entrenamiento e intente converger a parámetros óptimos.\n",
    "  - **El ciclo de validación/prueba**: repita el conjunto de datos de prueba para comprobar si el rendimiento del modelo está mejorando.\n",
    "\n",
    " Familiaricémonos brevemente con algunos de los conceptos utilizados en el ciclo de entrenamiento.  Saltar adelante a\n",
    " ver la etiqueta `full-impl` del ciclo de optimización.\n",
    "\n",
    "Función de pérdida(loos)\n",
    "~~~~~~~~~~~~~~~~~\n",
    "Cuando se le presentan algunos datos de entrenamiento, es probable que nuestra red no entrenada no proporcione la información correcta.\n",
    "responder. **Función de pérdida** mide el grado de disimilitud del resultado obtenido con el valor objetivo,\n",
    "y es la función de pérdida la que queremos minimizar durante el entrenamiento. Para calcular la pérdida hacemos un\n",
    "predicción utilizando las entradas de nuestra muestra de datos dada y compararla con el valor real de la etiqueta de datos.\n",
    "\n",
    "Las funciones de pérdida comunes incluyen `nn.MSELoss <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss>`_ (Error cuadrático medio) para tareas de regresión y\n",
    "`nn.NLLLoss <https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>`_ (Probabilidad de registro negativo) para la clasificación.\n",
    "`nn.CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss>`_ combina ``nn.LogSoftmax`` y ``nn.NLLLoss` `.\n",
    "\n",
    "Pasamos los logits de salida de nuestro modelo a ``nn.CrossEntropyLoss``, que normalizará los logits y calculará el error de predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "La optimización es el proceso de ajustar los parámetros del modelo para reducir el error del modelo en cada paso de entrenamiento. **Algoritmos de optimización** definen cómo se lleva a cabo este proceso (en este ejemplo usamos Stochastic Gradient Descent).\n",
    "Toda la lógica de optimización está encapsulada en el objeto ``optimizador``. Aquí, usamos el optimizador SGD; además, hay muchos `diferentes optimizadores <https://pytorch.org/docs/stable/optim.html>`_\n",
    "disponible en PyTorch como ADAM y RMSProp, que funcionan mejor para diferentes tipos de modelos y datos.\n",
    "\n",
    "Inicializamos el optimizador registrando los parámetros del modelo que necesitan ser entrenados y pasando el hiperparámetro de tasa de aprendizaje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del ciclo de entrenamiento, la optimización ocurre en tres pasos:\n",
    "  * Llame a ``optimizer.zero_grad()`` para restablecer los gradientes de los parámetros del modelo. Los degradados por defecto se suman; para evitar el doble conteo, los ponemos a cero explícitamente en cada iteración.\n",
    "  * Retropropaga la pérdida de predicción con una llamada a ``loss.backward()``. PyTorch deposita los gradientes de pérdida w.r.t. cada parámetro.\n",
    "  * Una vez que tenemos nuestros gradientes, llamamos a ``optimizer.step()`` para ajustar los parámetros por los gradientes recolectados en el paso hacia atrás(backward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plena aplicación\n",
    "-----------------------\n",
    "Definimos ``train_loop`` que recorre nuestro código de optimización, y ``test_loop`` que\n",
    "evalúa el rendimiento del modelo contra nuestros datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la función de pérdida y el optimizador, y lo pasamos a ``train_loop`` y ``test_loop``.\n",
    "Siéntase libre de aumentar el número de épocas para realizar un seguimiento de la mejora del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias A:\n",
    "----------------\n",
    "Documentacion oficial de pytorch: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "Traducido por: Mi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
