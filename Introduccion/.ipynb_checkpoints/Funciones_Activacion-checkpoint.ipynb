{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cda5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40640bd4",
   "metadata": {},
   "source": [
    "## Formula basica\n",
    " output = w*x + b\n",
    " \n",
    " output = activation_function(output)\n",
    " \n",
    " ### Antes de emprezar tenes que tner en cuenta que:\n",
    " \n",
    " **nn.ReLU()** crea un **nn.Module** que puede agregar, por ejemplo, a un modelo nn.Sequential.\n",
    " \n",
    " **torch.relu** en el otro lado es solo la llamada API funcional a la función relu,\n",
    " para que pueda agregarlo, por ejemplo, en su método de avance usted mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7339aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99ab7789",
   "metadata": {},
   "source": [
    "## Linear: \n",
    "Una función de línea recta donde la activación es proporcional a la entrada (que es la suma ponderada de la neurona).\n",
    "\n",
    "**Fomula matematica de la derivada(codigo):**\n",
    "\n",
    "`def linear_prime(z,m):\n",
    "\treturn m`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_prime.png)\n",
    "\n",
    "**Fomula matematica de la funcion(codigo):**\n",
    "\n",
    "`def linear(z,m):\n",
    "\treturn m*z`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear.png)\n",
    "\n",
    "\n",
    "**Ventajas:**\n",
    " - Da un rango de activaciones, por lo que no es una activación binaria. \n",
    " - Definitivamente podemos conectar algunas neuronas juntas y si se dispara más de 1, podríamos tomar el máximo (o softmax) y decidir en base a eso. \n",
    "\n",
    "**Desventajas:**\n",
    " - Para esta función, la derivada es una constante. Eso significa que el gradiente no tiene relación con X. \n",
    " - Es una pendiente constante y el descenso va a ser en pendiente constante.\n",
    " - Si hay un error en la predicción, los cambios realizados por la retropropagación son constantes y no dependen del cambio en la entrada delta(x). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "930873bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.2715, -0.6989, -0.1908,  0.5789])\n",
      "tensor([0.1956, 0.6300, 2.4629, 1.0383], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Si quisieramos una sola salida \"torch.randn(4, 4\"\n",
    "weight = torch.randn(4, 4) # Calculamos un peso aleatorio\n",
    "output = F.linear(input=x, weight=weight, bias=None) #Las salidas dependeran de las filas del peso(4filas = 4salidas)\n",
    "print(output)\n",
    "\n",
    "# Si quisieramos una sola salida \"L = nn.Linear(4, 1)\"\n",
    "L = nn.Linear(4, 4)\n",
    "output = L(x)\n",
    "print(output)\n",
    "\n",
    "#Los resultados son diferentes por que los pesos son randon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46264074",
   "metadata": {},
   "source": [
    "## sofmax\n",
    "\n",
    "La función Softmax calcula la distribución de probabilidades del evento sobre 'n' eventos diferentes. En términos generales, esta función calculará las probabilidades de cada clase objetivo sobre todas las clases objetivo posibles. Posteriormente, las probabilidades calculadas serán útiles para determinar la clase objetivo para las entradas dadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da9526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n",
      "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n"
     ]
    }
   ],
   "source": [
    "output = torch.softmax(x, dim=0)\n",
    "print(output)\n",
    "sm = nn.Softmax(dim=0)\n",
    "output = sm(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb21c7",
   "metadata": {},
   "source": [
    "## sigmoid \n",
    "\n",
    "Sigmoid toma un valor real como entrada y genera otro valor entre 0 y 1. Es fácil trabajar con él y tiene todas las buenas propiedades de las funciones de activación: no es lineal, continuamente diferenciable, monótono y tiene un rango de salida fijo.\n",
    "\n",
    "**Formula Matematica de la derivada(codigo):**\n",
    "\n",
    "`def sigmoid_prime(z):\n",
    "  return sigmoid(z) * (1-sigmoid(z))`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid_prime.png)\n",
    "\n",
    "**Formula Matematica de la funcion(codigo):**\n",
    "\n",
    "`def sigmoid(z):\n",
    "  return 1.0 / (1 + np.exp(-z))`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid.png)\n",
    "\n",
    "**Ventajas:**\n",
    " - Es de naturaleza no lineal. ¡Las combinaciones de esta función también son no lineales! \n",
    " - Dará una activación analógica a diferencia de la función de paso.\n",
    " - También tiene un degradado suave.\n",
    " - Es bueno para un clasificador.\n",
    " - La salida de la función de activación siempre estará en el rango (0,1) en comparación con (-inf, inf) de la función lineal. Entonces tenemos nuestras activaciones limitadas en un rango. Genial, entonces no explotará las activaciones. \n",
    "\n",
    "**Desventajas:**\n",
    " - Hacia cualquier extremo de la función sigmoidea, los valores de Y tienden a responder mucho menos a los cambios en X. \n",
    " - Da lugar a un problema de “gradientes que se desvanecen”. \n",
    " - Su salida no está centrada en cero. Hace que las actualizaciones de gradiente vayan demasiado lejos en diferentes direcciones. 0 <salida <1, y dificulta la optimización. \n",
    " - Los sigmoides saturan y eliminan los gradientes. \n",
    " - La red se niega a seguir aprendiendo o es drásticamente lenta (dependiendo del caso de uso y hasta que el gradiente/cálculo se vea afectado por los límites de valor de coma flotante). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342c520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n",
      "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n"
     ]
    }
   ],
   "source": [
    "output = torch.sigmoid(x)\n",
    "print(output)\n",
    "s = nn.Sigmoid()\n",
    "output = s(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6075122",
   "metadata": {},
   "source": [
    "## tanh\n",
    "\n",
    "Tanh reduce un número de valor real al rango [-1, 1]. No es lineal. Pero a diferencia de Sigmoid, su salida está centrada en cero. Por lo tanto, en la práctica siempre se prefiere la no linealidad tanh a la no linealidad sigmoidea.\n",
    "\n",
    "**Formula Matematica de la derivada(codigo):**\n",
    "\n",
    "`def tanh_prime(z):\n",
    "\treturn 1 - np.power(tanh(z), 2)`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh_prime.png)\n",
    "\n",
    "**Formula Matematica de la funcion(codigo):**\n",
    "\n",
    "`def tanh(z):\n",
    "\treturn (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh.png)\n",
    "\n",
    "**Ventajas:**\n",
    " - El gradiente es más fuerte para tanh que sigmoide (los derivados son más pronunciados).\n",
    "\n",
    "**Desventajas:**\n",
    " - Tanh también tiene el problema del gradiente de fuga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30cba6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n",
      "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n"
     ]
    }
   ],
   "source": [
    "output = torch.tanh(x)\n",
    "print(output)\n",
    "t = nn.Tanh()\n",
    "output = t(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cac9bb",
   "metadata": {},
   "source": [
    "## relu\n",
    "\n",
    "Un invento reciente que significa Unidades Lineales Rectificadas. La fórmula es engañosamente simple: max(0,z). A pesar de su nombre y apariencia, no es lineal y brinda los mismos beneficios que Sigmoid (es decir, la capacidad de aprender funciones no lineales), pero con un mejor rendimiento.\n",
    "\n",
    "**Formula Matematica de la derivada(codigo):**\n",
    "\n",
    "`def relu_prime(z):\n",
    "  return 1 if z > 0 else 0`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu_prime.png)\n",
    "\n",
    "**Formula Matematica de la funcion(codigo):**\n",
    "\n",
    "`def relu(z):\n",
    "  return max(0, z)`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu.png)\n",
    "\n",
    "**Ventajas:**\n",
    " - Evita y corrige el problema del gradiente de fuga. \n",
    " - ReLu es menos costoso computacionalmente que tanh y sigmoid porque involucra operaciones matemáticas más simples. \n",
    "\n",
    "**Desventajas:**\n",
    " - Una de sus limitaciones es que solo debe usarse dentro de capas ocultas de un modelo de red neuronal.\n",
    " - Algunos gradientes pueden ser frágiles durante el entrenamiento y pueden morir. Puede causar una actualización de peso que hará que nunca más se active en ningún punto de datos. En otras palabras, ReLu puede resultar en neuronas muertas. \n",
    " - En otras palabras, para activaciones en la región (x<0) de ReLu, el gradiente será 0 por lo que los pesos no se ajustarán durante el descenso. Eso significa que aquellas neuronas que entren en ese estado dejarán de responder a las variaciones en el error/entrada (simplemente porque el \n",
    " - El rango de ReLu es [0,∞). Esto significa que puede explotar la activación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0586f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "output = torch.relu(x)\n",
    "print(output)\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86081208",
   "metadata": {},
   "source": [
    "## leaky relu\n",
    "\n",
    "LeakyRelu es una variante de ReLU. En lugar de ser 0 cuando z<0, \n",
    "una ReLU con fugas permite un gradiente pequeño, distinto de cero y constante α(Normalmente, α=0.01). Sin embargo, la consistencia del beneficio entre tareas no está clara actualmente.\n",
    "\n",
    "**Formula Matematica de la derivada(codigo):**\n",
    "\n",
    "`def leakyrelu_prime(z, alpha):\n",
    "\treturn 1 if z > 0 else alpha`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu_prime.png)\n",
    "\n",
    "**Formula Matematica de la funcion(codigo):**\n",
    "\n",
    "`def leakyrelu(z, alpha):\n",
    "\treturn max(alpha * z, z`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu.png)\n",
    "\n",
    "**Ventajas:**\n",
    " - Las ReLU con fugas son un intento de solucionar el problema de la \"ReLU moribunda\" al tener una pequeña pendiente negativa (de 0,01, más o menos).  \n",
    "\n",
    "**Desventajas:**\n",
    " - Como posee linealidad, no puede ser utilizado para la Clasificación compleja. Va a la zaga de Sigmoid y Tanh en algunos de los casos de uso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb353cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n",
      "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n"
     ]
    }
   ],
   "source": [
    "output = F.leaky_relu(x)\n",
    "print(output)\n",
    "lrelu = nn.LeakyReLU()\n",
    "output = lrelu(x)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df6c9579",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "La unidad lineal exponencial o su nombre ampliamente conocido ELU es una función que tiende a hacer converger el costo a cero más rápido y produce resultados más precisos. A diferencia de otras funciones de activación, ELU tiene una constante alfa adicional que debería ser un número positivo.\n",
    "\n",
    "ELU es muy similar a RELU excepto entradas negativas. Ambos están en forma de función de identidad para entradas no negativas. Por otro lado, ELU se suaviza lentamente hasta que su salida es igual a -α mientras que RELU se suaviza bruscamente.\n",
    "\n",
    "**Formula Matematica de la derivada(codigo):**\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu_prime.png)\n",
    "\n",
    "`def elu_prime(z,alpha):\n",
    "\treturn 1 if z > 0 else alpha*np.exp(z)`\n",
    "\n",
    "**Formula Matematica de la funcion(codigo):**\n",
    "\n",
    "`def elu(z,alpha):\n",
    "\treturn z if z >= 0 else alpha*(e^z -1)`\n",
    "\n",
    "![imagen.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu.png)\n",
    "\n",
    "**Ventajas:**\n",
    " - ELU se suaviza lentamente hasta que su salida es igual a -α mientras que RELU se suaviza bruscamente.\n",
    " - ELU es una fuerte alternativa a ReLU. \n",
    " - A diferencia de ReLU, ELU puede producir salidas negativas.\n",
    "\n",
    "**Desventajas:**\n",
    " - Para x > 0, puede explotar la activación con el rango de salida de [0, inf]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "887c8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6321,  1.0000,  2.0000,  3.0000])\n",
      "tensor([-0.6321,  1.0000,  2.0000,  3.0000])\n"
     ]
    }
   ],
   "source": [
    "output = F.elu(x)\n",
    "print(output)\n",
    "\n",
    "el = nn.ELU()\n",
    "output = el(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97290bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
