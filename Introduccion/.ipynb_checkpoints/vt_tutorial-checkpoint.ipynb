{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages (from torch) (4.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/monky/jupyter_dir/jupyter_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Resultado_red' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4986/3608602326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mELU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mResultado_red\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Resultado_red' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "a = [[1,2],[3,4]]\n",
    "m = nn.ELU(a,alpha=1.0, inplace=Resultado_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Optimización del modelo de transformador de visión para la implementación\n",
    "===========================\n",
    "\n",
    "`Jeff Tang <https://github.com/jeffxtang>`_,\n",
    "`Geeta Chauhan <https://github.com/gchauhan/>`_\n",
    "\n",
    "Los modelos Vision Transformer aplican la tecnología de vanguardia basada en la atención\n",
    "modelos transformadores, introducidos en el procesamiento del lenguaje natural para lograr\n",
    "todo tipo de resultados de última generación (SOTA), hasta Computer Vision\n",
    "tasks. Facebook Data-efficient Image Transformers `DeiT <https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>`_\n",
    "es un modelo de Vision Transformer entrenado en ImageNet para la imagen\n",
    "clasificación.\n",
    "\n",
    "En este tutorial, primero cubriremos qué es DeiT y cómo usarlo,\n",
    "luego siga los pasos completos de secuencias de comandos, cuantificación, optimización,\n",
    "y usando el modelo en aplicaciones iOS y Android.  También compararemos la\n",
    "rendimiento de cuantificado, optimizado y no cuantificado, no optimizado\n",
    "modelos, y mostrar los beneficios de aplicar cuantización y optimización\n",
    "al modelo a lo largo de los pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es DeiT?\n",
    "---------------------\n",
    "\n",
    "Las redes neuronales convolucionales (CNN) han sido los principales modelos para la\n",
    "clasificación desde que el aprendizaje profundo despegó en 2012, pero las CNN suelen\n",
    "requieren cientos de millones de imágenes para el entrenamiento para lograr el\n",
    "Resultados de SOTA.  DeiT es un modelo de transformador de visión que requiere mucho menos\n",
    "datos y recursos informáticos para la formación para competir con los principales\n",
    "CNN en la realización de la clasificación de imágenes, que es posible gracias a dos\n",
    "Componentes clave de DeiT:\n",
    "\n",
    "-  Aumento de datos que simula el entrenamiento en un conjunto de datos mucho más grande;\n",
    "-  Destilación nativa que permite que la red de transformadores aprenda de\n",
    "   una salida de CNN.\n",
    "\n",
    "DeiT muestra que los transformadores se pueden aplicar con éxito a la computadora\n",
    "tareas de visión, con acceso limitado a datos y recursos.  Para más\n",
    "detalles sobre DeiT, consulte el`repo <https://github.com/facebookresearch/deit>`_\n",
    "y `paper <https://arxiv.org/abs/2012.12877>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificación de imágenes con DeiTiT\n",
    "-------------------------------\n",
    "\n",
    "Siga el README en el repositorio de DeiT para obtener información detallada sobre cómo\n",
    "clasificar imágenes usando DeiT, o para una prueba rápida, primero instale el\n",
    "paquetes requeridos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install timm\n",
    "#!pip install pandas\n",
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar en Google Colab, elimine el comentario de la siguiente línea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm pandas requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "luego ejecute el siguiente script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to /home/monky/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /home/monky/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a234eda7d3654c27aad0a3ac3c984881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(torch.__version__)\n",
    "# debiera ser 1.8.0\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
    "img = transform(img)[None,]\n",
    "out = model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida debe ser 269, que, de acuerdo con la lista de clase de ImageNet\n",
    "índice de `labels file <https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_, mapas a 'madera\n",
    " lobo, lobo gris, lobo gris, Canis lupus’.\n",
    "\n",
    "Ahora que hemos verificado que podemos usar el modelo DeiT para clasificar\n",
    " imágenes, veamos cómo modificar el modelo para que pueda ejecutarse en iOS y\n",
    " aplicaciones de Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripting DeiT\n",
    "----------------------\n",
    "\n",
    "Para usar el modelo en dispositivos móviles, primero debemos crear una secuencia de comandos\n",
    "modelo. Consulte la receta `Script and Optimize <https://pytorch.org/tutorials/recipes/script_optimized.html>`_ para obtener una\n",
    "vista rápida.  Ejecute el siguiente código para convertir el modelo DeiT utilizado en el\n",
    "paso previo al formato TorchScript que se puede ejecutar en el móvil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/monky/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"fbdeit_scripted.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de modelo con script fbdeit_scripted.pt de un tamaño aproximado de 346 MB es\n",
    "generado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing DeiT\n",
    "---------------------\n",
    "Para reducir significativamente el tamaño del modelo entrenado mientras\n",
    "manteniendo la precisión de la inferencia casi igual, la cuantificación puede ser\n",
    "aplicado al modelo.  Gracias al modelo de transformador utilizado en DeiT,\n",
    "puede aplicar fácilmente la cuantificación dinámica al modelo, porque la dinámica\n",
    "la cuantificación funciona mejor para LSTM y modelo de transformador (ver `here <https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization>`_\n",
    "para más detalles).\n",
    "\n",
    "Ahora ejecuta el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages/torch/ao/quantization/observer.py:177: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use 'fbgemm' para la inferencia del servidor y 'qnnpack' para la inferencia móvil\n",
    "backend = \"fbgemm\" # reemplazado con qnnpack que causa una velocidad de inferencia mucho peor para el modelo cuantificado en este portátil\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "scripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto genera la versión con guión y cuantificada del modelo.\n",
    "\n",
    "fbdeit_quantized_scripted.pt, con un tamaño aproximado de **89 MB**, una reducción del **74%** de\n",
    "¡el tamaño del modelo no cuantificado de **346 MB!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede usar ``scripted_quantized_model`` para generar el mismo\n",
    "resultado de la inferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())\n",
    "# Se debe imprimir la misma salida 269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizando DeiT\n",
    "---------------------\n",
    "El paso final antes de usar la cuantificación y el guión\n",
    "modelo en el móvil es optimizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
    "optimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo **fbdeit_optimized_scripted_quantized.pt** generado tiene aproximadamente el\n",
    "mismo tamaño que el modelo cuantificado, con guión, pero no optimizado. los\n",
    "resultado de la inferencia sigue siendo el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1463.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "out = optimized_scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())\n",
    "# Nuevamente, se debe imprimir la misma salida 269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el intérprete ligero\n",
    "------------------------\n",
    "\n",
    "Para ver cuánto la reducción del tamaño del modelo y la inferencia aceleran el Lite\n",
    "El intérprete puede resultar en, vamos a crear la versión lite del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n",
    "ptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el tamaño del modelo lite es comparable a la versión no lite, cuando\n",
    "ejecutando la versión lite en el móvil, se espera que la inferencia se acelere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de la velocidad de inferencia\n",
    "---------------------------\n",
    "\n",
    "Para ver cómo difiere la velocidad de inferencia para los cuatro modelos, el\n",
    "modelo original, el modelo con guión, el modelo cuantificado y con guión,\n",
    "el modelo optimizado, cuantificado y con secuencias de comandos: ejecute el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model: 992.68ms\n",
      "scripted model: 1470.35ms\n",
      "scripted & quantized model: 939.59ms\n",
      "scripted & quantized & optimized model: 346.83ms\n",
      "lite model: 277.17ms\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
    "    out = model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
    "    out = scripted_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
    "    out = scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
    "    out = optimized_scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
    "    out = ptl(img)\n",
    "\n",
    "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
    "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
    "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results summarize the inference time taken by each model\n",
    "and the percentage reduction of each model relative to the original\n",
    "model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Model Inference Time Reduction\n",
      "0                          original model       992.68ms        0%\n",
      "1                          scripted model      1470.35ms   -48.12%\n",
      "2              scripted & quantized model       939.59ms     5.35%\n",
      "3  scripted & quantized & optimized model       346.83ms    65.06%\n",
      "4                              lite model       277.17ms    72.08%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n        Model                            Tiempo de inferencia    Reducción\\n0\\tmodelo original                                 1236.69ms           0%\\n1\\tmodelo guionizado                               1226.72ms        0.81%\\n2\\tmodelo con guión y cuantificado                  593.19ms       52.03%\\n3\\tmodelo con guión, cuantificado y optimizado      598.01ms       51.64%\\n4\\tmodelo ligero                                    600.72ms       51.43%\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n",
    "df = pd.concat([df, pd.DataFrame([\n",
    "    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
    "    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n",
    "    columns=['Tiempo de inferencia', 'Reducción'])], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de Quantizing:\n",
    "---------------------\n",
    "### Eager Mode Quantization / Cuantificación del modo entusiasta:\n",
    " - **Post Training Dynamic Quantization / Dinámica posterior al entrenamiento Cuantificación**\n",
    " - **Post Training Static Quantization / Estática posterior al entrenamiento Cuantificación**\n",
    " - **Quantization Aware Training for Static Quantization / Cuantificación para la cuantificación estática**\n",
    "\n",
    "### FX Graph Mode Quantization / Gráfico de efectos Modo cuantización:\n",
    " - **FX Graph Mode Quantization / Cuantificación del modo gráfico de efectos**\n",
    " - **FX Graph Mode Post Training Static Quantization / Modo gráfico de efectos Cuantificación estática posterior al entrenamiento**\n",
    " - **FX Graph Mode Post Training Dynamic Quantization / Modo gráfico FX Cuantificación dinámica posterior al entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparación del modelo para la cuantificación estática en modo Ansioso**\n",
    "\n",
    " - Convierta cualquier operación que requiera volver a cuantificar (y por lo tanto tenga parámetros adicionales) de funcionales a forma de módulo (por ejemplo, usando `torch.nn.ReLU` en vez de `torch.nn.functional.relu`). \n",
    " -  Especifique qué partes del modelo deben cuantificarse asignando `.qconfig` atributos en submódulos o especificando `qconfig_dict`. Por ejemplo, establecer `model.conv1.qconfig = None` significa que el `model.conv` la capa no se cuantizará y el ajuste `model.linear1.qconfig = custom_qconfig` significa que la cuantización ajustes para `model.linear1` estará usando `custom_qconfigen` cambio del qconfig global. \n",
    "\n",
    "Para las técnicas de cuantificación estática que cuantifican las activaciones, el usuario debe hacer lo siguiente además:\n",
    "\n",
    " - Especifique dónde se cuantifican y descuantifican las activaciones. Esto se hace usando `QuantStub`y `DeQuantStub`módulos.\n",
    " - Usar `torch.nn.quantized.FloatFunctional` para envolver operaciones tensoriales que requieren un manejo especial para la cuantificación en módulos. Ejemplos son operaciones como `add` y `cat` que requieren un manejo especial para determinar los parámetros de cuantificación de salida. \n",
    " \n",
    " -Módulos de fusibles: combine operaciones/módulos en un solo módulo para obtener mayor precisión y rendimiento. Esto se hace usando el torch.quantization.fuse_modules()API, que toma listas de módulos estar fusionado. Actualmente admitimos las siguientes fusiones: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Lineal, Relu]\n",
    " \n",
    " También aquí hay otra lista de los métodos de fusión: \n",
    " \n",
    "`DEFAULT_OP_LIST_TO_FUSER_METHOD : Dict[Tuple, Union[nn.Sequential, Callable]] = {\n",
    "(nn.Conv1d, nn.BatchNorm1d): fuse_conv_bn,\n",
    "(nn.Conv1d, nn.BatchNorm1d, nn.ReLU): fuse_conv_bn_relu,\n",
    "(nn.Conv2d, nn.BatchNorm2d): fuse_conv_bn,\n",
    "(nn.Conv2d, nn.BatchNorm2d, nn.ReLU): fuse_conv_bn_relu,\n",
    "(nn.Conv3d, nn.BatchNorm3d): fuse_conv_bn,\n",
    "(nn.Conv3d, nn.BatchNorm3d, nn.ReLU): fuse_conv_bn_relu,\n",
    "(nn.Conv1d, nn.ReLU): nni.ConvReLU1d,\n",
    "(nn.Conv2d, nn.ReLU): nni.ConvReLU2d,\n",
    "(nn.Conv3d, nn.ReLU): nni.ConvReLU3d,\n",
    "(nn.Linear, nn.BatchNorm1d): fuse_linear_bn,\n",
    "(nn.Linear, nn.ReLU): nni.LinearReLU,\n",
    "(nn.BatchNorm2d, nn.ReLU): nni.BNReLU2d,\n",
    "(nn.BatchNorm3d, nn.ReLU): nni.BNReLU3d,}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Mode Quantization / Cuantificación del modo entusiasta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post Training Dynamic Quantization / Dinámica posterior al entrenamiento Cuantificación**\n",
    "\n",
    "más simple de aplicar cuantización donde los pesos son cuantificado antes de tiempo, pero las activaciones se cuantifican dinámicamente durante la inferencia. Esto se utiliza para situaciones en las que el tiempo de ejecución del modelo está dominado por la carga de pesos de la memoria en lugar de calcular la matriz multiplicaciones Esto es cierto para los modelos de tipo LSTM y Transformador con tamaño de lote pequeño.\n",
    "\n",
    "modelo original\n",
    "todos los tensores y cálculos están en punto flotante:\n",
    "\n",
    "`previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 = linear_weight_fp32`\n",
    "\n",
    "modelo cuantificado dinámicamente\n",
    "los pesos lineales y LSTM están en int8:\n",
    "\n",
    "`previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32 = linear_weight_int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Definir un modelo de coma flotante\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        self.fc = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Crear una instancia de modelo\n",
    "model_fp32 = M()\n",
    "\n",
    "# Crear una instancia de modelo cuantificada\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,         # el modelo original\n",
    "    {torch.nn.Linear},  # un conjunto de capas para cuantificar dinámicamente\n",
    "    dtype=torch.qint8)  # el tipo de d objetivo para pesos cuantificados\n",
    "\n",
    "# ejecutar el modelo\n",
    "input_fp32 = torch.randn(4, 4, 4, 4)\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post Training Static Quantization / Estática posterior al entrenamiento Cuantificación**\n",
    "\n",
    "Post Training Static Quantization (PTQ estático) cuantifica los pesos y las activaciones del modelo. Eso fusiona activaciones en capas anteriores cuando es posible. Requiere calibración con un conjunto de datos representativo para determinar la cuantificación óptima parámetros para activaciones. La cuantificación estática posterior al entrenamiento se usa normalmente cuando tanto el ancho de banda de la memoria como los ahorros informáticos son importantes, ya que las CNN son un caso de uso típico.\n",
    "\n",
    "modelo original\n",
    "todos los tensores y cálculos están en punto flotante:\n",
    "\n",
    "`previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 = linear_weight_fp32`\n",
    "\n",
    "modelo cuantificado estáticamente\n",
    "pesos y activaciones están en int8\n",
    "\n",
    "`previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 = linear_weight_int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monky/jupyter_dir/jupyter_env/lib/python3.9/site-packages/torch/ao/quantization/observer.py:177: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# definir un modelo de punto flotante donde algunas capas podrían cuantificarse estáticamente\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        # QuantStub convierte tensores de coma flotante a cuantizados\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub convierte tensores de cuantizados a punto flotante\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # especificar manualmente dónde se convertirán los tensores de flotantes\n",
    "        # apuntar a cuantizado en el modelo cuantizado\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # especificar manualmente dónde se convertirán los tensores de cuantificados\n",
    "        # a punto flotante en el modelo cuantizado\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# crear una instancia de modelo\n",
    "model_fp32 = M()\n",
    "\n",
    "# el modelo debe configurarse en modo eval para que funcione la lógica de cuantificación estática\n",
    "model_fp32.eval()\n",
    "\n",
    "# adjunte un qconfig global, que contiene información sobre qué tipo\n",
    "# de observadores a adjuntar. Use 'fbgemm' para la inferencia del servidor y\n",
    "# 'qnnpack' para inferencia móvil. Otras configuraciones de cuantificación como\n",
    "# como seleccionar cuantización simétrica o asimétrica y MinMax o L2Norm\n",
    "# Aquí se pueden especificar # técnicas de calibración.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fusionar las activaciones con las capas anteriores, cuando corresponda.\n",
    "# Esto debe hacerse manualmente según la arquitectura del modelo.\n",
    "# Las fusiones comunes incluyen `conv + relu` y `conv + batchnorm + relu`\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare el modelo para la cuantificación estática. Esto inserta a los observadores en\n",
    "# el modelo que observará los tensores de activación durante la calibración.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrar el modelo preparado para determinar los parámetros de cuantificación para las activaciones\n",
    "# en un escenario del mundo real, la calibración se haría con un conjunto de datos representativo\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convierta el modelo observado en un modelo cuantificado. Esto hace varias cosas:\n",
    "# cuantifica los pesos, calcula y almacena el valor de escala y sesgo que se\n",
    "# usado con cada tensor de activación, y reemplaza operadores clave con operadores cuantificados\n",
    "# implementaciones.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# ejecute el modelo, los cálculos relevantes ocurrirán en int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization Aware Training for Static Quantization / Cuantificación para la cuantificación estática**\n",
    "\n",
    "Quantization Aware Training (QAT) modela los efectos de la cuantificación durante el entrenamiento lo que permite una mayor precisión en comparación con otros métodos de cuantificación. Podemos hacer QAT para cuantificación estática, dinámica o solo de peso. Durante entrenamiento, todos los cálculos se realizan en coma flotante, con módulos fake_quant modelar los efectos de la cuantización mediante sujeción y redondeo para simular el efectos de INT8. Después de la conversión del modelo, los pesos y las activaciones se cuantifican y las activaciones se fusionan en la capa anterior donde sea posible. Se usa comúnmente con CNN y produce una mayor precisión. en comparación con la cuantificación estática.\n",
    "\n",
    "Es posible que necesitemos modificar el modelo antes de aplicar la cuantificación. [Consulte la preparación del modelo para la cuantificación estática en modo ansioso](https://pytorch.org/docs/stable/quantization.html#model-preparation-for-eager-mode-static-quantization)\n",
    "\n",
    "\n",
    "modelo original todos los tensores y cálculos están en punto flotante:\n",
    "\n",
    "`previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 = linear_weight_fp32`\n",
    "\n",
    "modelo con fake_quants para modelar valores numéricos de cuantización durante el entrenamiento:\n",
    "\n",
    "`previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32 = linear_weight_fp32 -- fq`\n",
    "\n",
    "modelo cuantificado pesos y activaciones están en int8:\n",
    "\n",
    "`previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 = linear_weight_int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# definir un modelo de punto flotante donde algunas capas podrían beneficiarse de QAT\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        # QuantStub convierte tensores de coma flotante a cuantizados\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub convierte tensores de cuantizados a punto flotante\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# crear una instancia de modelo\n",
    "model_fp32 = M()\n",
    "\n",
    "# el modelo debe configurarse en modo de entrenamiento para que funcione la lógica QAT\n",
    "model_fp32.train()\n",
    "\n",
    "#adjunte un qconfig global, que contiene información sobre qué tipo\n",
    "#de observadores a adjuntar. Use 'fbgemm' para la inferencia del servidor y\n",
    "#'qnnpack' para inferencia móvil. Otras configuraciones de cuantificación como\n",
    "#como seleccionar cuantización simétrica o asimétrica y MinMax o L2Norm\n",
    "#aquí se pueden especificar las técnicas de calibración.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "###########################\n",
    "model_fp32.eval()\n",
    "###########################\n",
    "\n",
    "# fusionar las activaciones con las capas anteriores, cuando corresponda\n",
    "# esto debe hacerse manualmente dependiendo de la arquitectura del modelo\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n",
    "    [['conv', 'bn', 'relu']])\n",
    "\n",
    "############################\n",
    "model_fp32_fused.train()\n",
    "############################\n",
    "\n",
    "# Prepare el modelo para QAT.  Esto inserta observadores y fake_quants en\n",
    "# el modelo que observará el peso y los tensores de activación durante la calibración.\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "# ejecutar el ciclo de entrenamiento (no se muestra)\n",
    "#training_loop(model_fp32_prepared)\n",
    "\n",
    "# Convierta el modelo observado en un modelo cuantificado. Esto hace varias cosas:\n",
    "# cuantifica los pesos, calcula y almacena la escala y el valor de sesgo para ser\n",
    "# utilizados con cada tensor de activación, módulos de fusibles en su caso,\n",
    "# y reemplaza a los operadores clave con implementaciones cuantificadas.\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "# ejecute el modelo, los cálculos relevantes ocurrirán en int8\n",
    "res = model_int8(input_fp32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX Graph Mode Quantization / Gráfico de efectos Modo cuantización:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aquí hay múltiples tipos de cuantificación en la cuantificación posterior al entrenamiento (solo peso, dinámica y estática) y la configuración se realiza a través de qconfig_dict (un argumento de la función prepare_fx).\n",
    "\n",
    "FX Graph Mode Quantization es un marco de cuantificación automatizado en PyTorch,\n",
    "y actualmente es una característica de prototipo. Mejora la cuantificación del modo ansioso al agregar soporte para funciones y automatizar el proceso de cuantificación, aunque es posible que las personas necesiten refactorizar el modelo para que sea compatible con la cuantificación del modo gráfico FX (simbólicamente rastreable con `torch.fx`). Tenga en cuenta que no se espera que FX Graph Mode Quantization funcione en modelos arbitrarios, ya que es posible que el modelo no se pueda rastrear simbólicamente, lo integraremos en bibliotecas de dominio como torchvision y los usuarios podrán cuantificar modelos similares a los de las bibliotecas de dominio compatibles con FX Cuantificación en modo gráfico. Para modelos arbitrarios, proporcionaremos pautas generales, pero para que realmente funcione, es posible que los usuarios deban estar familiarizados con `torch.fx`, especialmente sobre cómo hacer un modelo trazable simbólicamente.\n",
    "\n",
    "[Comparacion de \"FX Graph Mode Quantization\" vs \"Eager Mode Quantization\"](https://pytorch.org/docs/stable/quantization.html#quantization-flow-support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "class UserModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserModel, self).__init__()\n",
    "        # QuantStub convierte tensores de coma flotante a cuantizados\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub convierte tensores de cuantizados a punto flotante\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "model_fp = UserModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FX Graph Mode Quantization / Cuantificación del modo gráfico de efectos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# quantization aware training for static quantization\n",
    "#\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\n",
    "\n",
    "model_to_quantize.train() #Ponemos el modelo en modo de entrenamiento\n",
    "\n",
    "# prepare\n",
    "model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_dict)\n",
    "# training loop (not shown)\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "model_to_quantize.eval()\n",
    "model_fused = quantize_fx.fuse_fx(model_to_quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FX Graph Mode Post Training Static Quantization / Modo gráfico de efectos Cuantificación estática posterior al entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# post training static quantization\n",
    "#\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\n",
    "\n",
    "model_to_quantize.eval() #Ponemos el modelo en modo de evaluacion\n",
    "\n",
    "# preparar\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n",
    "# calibrar (no se muestra)\n",
    "# cuantificar\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "model_to_quantize.eval()\n",
    "model_fused = quantize_fx.fuse_fx(model_to_quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FX Graph Mode Post Training Dynamic Quantization / Modo gráfico FX Cuantificación dinámica posterior al entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# post training dynamic/weight_only quantization\n",
    "#\n",
    "\n",
    "# necesitamos una copia profunda si aún queremos mantener model_fp sin cambios después de la cuantificación, ya que las API de cuantificación cambian el modelo de entrada\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n",
    "\n",
    "model_to_quantize.eval() #Ponemos el modelo en modo de evaluacion\n",
    "\n",
    "# preparar\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n",
    "# no se necesita calibración cuando solo tenemos cuantificación dynamici/weight_only\n",
    "# cuantificar\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "model_to_quantize.eval()\n",
    "model_fused = quantize_fx.fuse_fx(model_to_quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API de módulo personalizado de cuantificación\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto el modo Eager/Ansioso como las API de cuantificación del modo gráfico FX proporcionan un gancho para el usuario para especificar el módulo cuantificado de forma personalizada, con lógica definida por el usuario para observación y cuantificación. El usuario debe especificar: \n",
    "\n",
    " - 1) El tipo Python del módulo fuente fp32 (existente en el modelo) \n",
    " - 2) El tipo de Python del módulo observado (proporcionado por el usuario). Este módulo necesita para definir una *from_float* que define cómo es el módulo observado creado a partir del módulo fp32 original. \n",
    " - 3) El tipo de Python del módulo cuantificado (proporcionado por el usuario). Este módulo necesita para definir una *from_observed* que define cómo se cuantifica el módulo creado a partir del módulo observado. \n",
    "\n",
    "Una configuración que describe (1), (2), (3) anterior, pasada a las API de cuantificación.\n",
    "El marco entonces hará lo siguiente: \n",
    "   - durante la preparación de intercambios de módulos, convertirá cada módulo de tipo especificado en (1) al tipo especificado en (2), usando la *from_float* función la clase en (2). \n",
    "   - durante la conversión , convertirá cada módulo de tipo especificado en (2) al tipo especificado en (3), usando la *from_observed* función de la clase en (3). \n",
    "   \n",
    "Actualmente, existe el requisito de que *ObservedCustomModule* tenga un solo Salida de tensor, y el marco agregará un observador (no el usuario) en esa salida. El observador se almacenará bajo la *activation_post_process* clave como un atributo de la instancia del módulo personalizado. Relajar estas restricciones puede hacerse en un momento futuro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.quantized as nnq\n",
    "import torch.quantization.quantize_fx\n",
    "\n",
    "# módulo fp32 original para reemplazar\n",
    "class CustomModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# módulo observado personalizado, proporcionado por el usuario\n",
    "class ObservedCustomModule(torch.nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, float_module):\n",
    "        assert hasattr(float_module, 'qconfig')\n",
    "        observed = cls(float_module.linear)\n",
    "        observed.qconfig = float_module.qconfig\n",
    "        return observed\n",
    "\n",
    "# custom quantized module, provided by user\n",
    "class StaticQuantCustomModule(torch.nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, observed_module):\n",
    "        assert hasattr(observed_module, 'qconfig')\n",
    "        assert hasattr(observed_module, 'activation_post_process')\n",
    "        observed_module.linear.activation_post_process = \\\n",
    "            observed_module.activation_post_process\n",
    "        quantized = cls(nnq.Linear.from_float(observed_module.linear))\n",
    "        return quantized\n",
    "\n",
    "#\n",
    "# ejemplo de llamada API (cuantificación del modo Eager/Ansioso)\n",
    "#\n",
    "\n",
    "m = torch.nn.Sequential(CustomModule()).eval()\n",
    "\n",
    "prepare_custom_config_dict = {\n",
    "    \"float_to_observed_custom_module_class\": {\n",
    "        CustomModule: ObservedCustomModule\n",
    "    }\n",
    "}\n",
    "convert_custom_config_dict = {\n",
    "    \"observed_to_quantized_custom_module_class\": {\n",
    "        ObservedCustomModule: StaticQuantCustomModule\n",
    "    }\n",
    "}\n",
    "\n",
    "m.qconfig = torch.quantization.default_qconfig\n",
    "mp = torch.quantization.prepare(\n",
    "    m, prepare_custom_config_dict=prepare_custom_config_dict)\n",
    "# calibración (no se muestra)\n",
    "mq = torch.quantization.convert(\n",
    "    mp, convert_custom_config_dict=convert_custom_config_dict)\n",
    "\n",
    "#\n",
    "# ejemplo de llamada API (cuantificación del modo gráfico FX)\n",
    "#\n",
    "\n",
    "m = torch.nn.Sequential(CustomModule()).eval()\n",
    "\n",
    "qconfig_dict = {'': torch.quantization.default_qconfig}\n",
    "prepare_custom_config_dict = {\n",
    "    \"float_to_observed_custom_module_class\": {\n",
    "        \"static\": {\n",
    "            CustomModule: ObservedCustomModule,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "convert_custom_config_dict = {\n",
    "    \"observed_to_quantized_custom_module_class\": {\n",
    "        \"static\": {\n",
    "            ObservedCustomModule: StaticQuantCustomModule,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "mp = torch.quantization.quantize_fx.prepare_fx(\n",
    "    m, qconfig_dict, prepare_custom_config_dict=prepare_custom_config_dict)\n",
    "# calibración (no se muestra)\n",
    "mq = torch.quantization.quantize_fx.convert_fx(\n",
    "    mp, convert_custom_config_dict=convert_custom_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejores prácticas\n",
    "\n",
    "Si está utilizando el `fbgemm` en el backend, necesitamos usar 7 bits en lugar de 8 bits. Asegúrese de reducir el alcance de la `quant\\_min`, `quant\\_max`, p.ej si `dtype` es `torch.quint8`, asegúrese de establecer un personalizado `quant_minser` - estar `0` y `quant_maxser` - estar `127(255 / 2)` si `dtype` es `torch.qint8`, asegúrese de establecer un personalizado `quant_minser` - estar `-64( -128 / 2)` y `quant_maxser` estar `63(127 / 2)`, ya configuramos esto correctamente si llama a la función *torch.ao.quantization.get_default_qconfig(backend)* o *torch.ao.quantization.get_default_qat_qconfig(backend)* para obtener el valor predeterminado `qconfig` por `fbgemm` o servidor `qnnpack `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias A:\n",
    "----------------\n",
    "Documentacion oficial de pytorch: \n",
    "- https://pytorch.org/docs/stable/quantization.html#quantization-custom-module-api\n",
    "\n",
    "- https://pytorch.org/docs/stable/quantization.html#quantization-api-summary\n",
    "\n",
    "Traducido por: Mi :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
