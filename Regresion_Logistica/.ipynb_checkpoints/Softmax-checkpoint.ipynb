{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e88464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d794fed3",
   "metadata": {},
   "source": [
    "## Cross entropy\n",
    "\n",
    "La entropía cruzada se usa comúnmente en el aprendizaje automático como una función de pérdida.\n",
    "\n",
    "La entropía cruzada es una medida del campo de la teoría de la información, que se basa en la entropía y generalmente calcula la diferencia entre dos distribuciones de probabilidad. Está estrechamente relacionado pero es diferente de divergencia KL que calcula la entropía relativa entre dos distribuciones de probabilidad, mientras que se puede pensar que la entropía cruzada calcula la entropía total entre las distribuciones.\n",
    "\n",
    "La entropía cruzada también está relacionada y, a menudo, se confunde con la pérdida logística, llamada pérdida de registro . Aunque las dos medidas se derivan de una fuente diferente, cuando se usan como funciones de pérdida para modelos de clasificación, ambas medidas calculan la misma cantidad y se pueden usar indistintamente. \n",
    "\n",
    "**Formula matematica :**\n",
    "![imagen.png](https://i.stack.imgur.com/gNip2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a14582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])\n",
    "\n",
    "# y debe ser una codificación en caliente\n",
    "# if class 0: [1 , 0, 0]\n",
    "# if class 0: [0 , 1, 0]\n",
    "# if class 0: [0 , 0, 1]\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "# Y_pred tiene probabilidades\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1]) # Aqui podemos ver que la distribucion de nuestras probabilidades es correcta\n",
    "Y_pred_bag = np.array([0.1, 0.3, 0.6])  # Aqui podemos ver que la distribucion de nuestras probabilidades es incorrecta\n",
    "\n",
    "l1 = cross_entropy(Y, Y_pred_good)# Almacenamos la perdida de nuestra primera probabilidad\n",
    "l2 = cross_entropy(Y, Y_pred_bag) # Almacenamos la perdida de nuestra segunda probabilidad\n",
    "\n",
    "print(f\"Loss1 numpy: {l1:.4f}\")# Nos da una perdida menor debido a nuestra buena distribucion de probabilidades\n",
    "print(f\"Loss2 numpy: {l2:.4f}\")# Nos da una perdida mayor debido a nuestra mala distribucion de probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86496a4b",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "La función softmax, también conocida como softargmax o función exponencial normalizada, es una generalización de la función logística a múltiples dimensiones. Se usa en la regresión logística multinomial y, a menudo, se usa como la última función de activación de una red neuronal para normalizar la salida de una red a una distribución de probabilidad sobre las clases de salida predichas, según el axioma de elección de Luce.\n",
    "\n",
    "**Formula matematica :**\n",
    "![imagen.png](https://miro.medium.com/max/1400/1*hwdjtUG2pv8EhuxcR4mWmA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f9a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(manual) numpy: [0.65900114 0.24243297 0.09856589]\n",
      "softmax(pytorch) numpy: tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "#crearmos una funcion softmax de manera manual\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print(f\"softmax(manual) numpy: {outputs}\") # imprimimos los resultados de nuestra funcion softmax\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(f\"softmax(pytorch) numpy: {outputs}\")# imprimimos los resultados de la funcion softmax(de pytorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cfd01",
   "metadata": {},
   "source": [
    "## Ejemplo manual: CrossEntropyLoss en PyTorch (aplica Softmax) aplicando clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e175de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Y_pred_good(Loss1): 0.4170\n",
      "PyTorch Y_pred_bad(Loss2): 1.8406\n",
      "clase real 0, Y_pred_good(Loss1): 0, Y_pred_bad(Loss2): 1\n"
     ]
    }
   ],
   "source": [
    "# nn.LogSoftmax + nn.NLLLoss\n",
    "# NLLLoss = pérdida de probabilidad logarítmica negativa\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# loss(input, target)\n",
    "\n",
    "# objetivo es de tamaño nSamples = 1\n",
    "# cada elemento tiene una etiqueta de clase: 0, 1 o 2\n",
    "# Y (=objetivo) contiene etiquetas de clase, no one-hot\n",
    "Y = torch.tensor([0])\n",
    "\n",
    "#la entrada tiene un tamaño de nMuestras x nClases = 1 x 3\n",
    "# y_pred (=entrada) debe ser sin procesar, no normaliza las puntuaciones (logits) para cada clase, no softmax\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])# Esta prediccion indica que el resultado com mayor probabilidad es 0(por que tiene la probabilidad mas alta en la primera posicion)\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]]) # Esta prediccion indica que el resultado com mayor probabilidad es 1(por que tiene la probabilidad mas alta en la segunda posicion)\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'PyTorch Y_pred_good(Loss1): {l1.item():.4f}')\n",
    "print(f'PyTorch Y_pred_bad(Loss2): {l2.item():.4f}')\n",
    "\n",
    "# obtener predicciones\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(f'clase real {Y.item()}, Y_pred_good(Loss1): {predictions1.item()}, Y_pred_bad(Loss2): {predictions2.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb480b",
   "metadata": {},
   "source": [
    "## Ejemplo manual: CrossEntropyLoss en PyTorch (aplica Softmax) aplicando múltiples muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd838bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Y_pred_good(Loss1):  0.2834\n",
      "Batch Y_pred_bad(Loss2): 1.6418\n",
      "Actual class: tensor([2, 0, 1]), Y_pred_good(Loss1): tensor([2, 0, 1]), Y_pred_bad(Loss2): tensor([0, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# el objetivo es de tamaño nBatch = 3\n",
    "# cada elemento tiene una etiqueta de clase: 0, 1 o 2\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "\n",
    "# la entrada tiene un tamaño nBatch x nClasses = 3 x 3\n",
    "# Y_pred son logits (no softmax)\n",
    "Y_pred_good = torch.tensor(\n",
    "    [[0.1, 0.2, 3.9], # predecir clase 2(en la primera posicion)\n",
    "    [1.2, 0.1, 0.3],  # predecir clase 0(en la segunda posicion) # ----> El resultado de esto [2,0,1]\n",
    "    [0.3, 2.2, 0.2]]) # predecir clase 1(en la tercera posicion)\n",
    "\n",
    "Y_pred_bad = torch.tensor(\n",
    "    [[0.9, 0.2, 0.1], # predecir clase 0(en la primera posicion)\n",
    "    [0.1, 0.3, 1.5],  # predecir clase 2(en la segunda posicion) # ----> El resultado de esto [0,2,0]\n",
    "    [1.2, 0.2, 0.5]]) # predecir clase 0(en la tercera posicion)\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "print(f'Batch Y_pred_good(Loss1):  {l1.item():.4f}')\n",
    "print(f'Batch Y_pred_bad(Loss2): {l2.item():.4f}')\n",
    "\n",
    "# obtener predicciones\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(f'Actual class: {Y}, Y_pred_good(Loss1): {predictions1}, Y_pred_bad(Loss2): {predictions2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca135600",
   "metadata": {},
   "source": [
    "## Red nueronal para clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52651d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss() #(NO aplica Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b1e1f",
   "metadata": {},
   "source": [
    "## Red nueronal para clasificación multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9381eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sin softmax al final(solo se usa en la clasificación binaria)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (aplica Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b9970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
