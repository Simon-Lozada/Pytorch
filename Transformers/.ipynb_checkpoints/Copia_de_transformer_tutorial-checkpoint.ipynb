{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0HzUZ3zgg05"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp9CZWxTgg08"
   },
   "source": [
    "\n",
    "Language Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "Este es un tutorial sobre cómo entrenar un modelo de secuencia a secuencia que usa el nn.Transformer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNgA2Yd_gg1B"
   },
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0avIa8lwgg1C"
   },
   "source": [
    "En este tutorial, entrenamos un modelo ``nn.TransformerEncoder`` en un\n",
    "tarea de modelado del lenguaje.  La tarea de modelado del lenguaje es asignar un\n",
    "probabilidad de la probabilidad de una palabra dada (o una secuencia de palabras)\n",
    "seguir una secuencia de palabras.  Una secuencia de tokens se pasa a la incrustación\n",
    "capa primero, seguida de una capa de codificación posicional para dar cuenta del orden\n",
    "de la palabra (ver el párrafo siguiente para más detalles).  los\n",
    "``nn.TransformerEncoder`` consta de varias capas de\n",
    "``nn.TransformerEncoderLayer``\n",
    "Junto con la secuencia de entrada, se requiere una máscara de atención cuadrada porque la\n",
    "las capas de autoatención en ``nn.TransformerEncoder`` solo pueden asistir\n",
    "las primeras posiciones de la secuencia.  Para la tarea de modelado del lenguaje, cualquier\n",
    "los tokens en las posiciones futuras deben estar enmascarados.  Para producir una probabilidad\n",
    "distribución sobre palabras de salida, la salida de ``nn.TransformerEncoder``\n",
    "El modelo se pasa a través de una capa lineal seguida de una función log-softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL6z7VEjgg1E"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'# Definimos nuestro tipo de modelo\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)# Iniciamos la clase \"PositionalEncoding\" en \"pos_encoder\"\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)# Creamos una red transformer de autoatención y alimentación directa \n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)# Apilamos nuestras capas \"encoder_layers\"\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)# hacemos una tabla de búsqueda simple que almacena incrustaciones de un diccionario y tamaño fijos. \n",
    "        self.d_model = d_model# El número de características esperadas en la entrada\n",
    "        self.decoder = nn.Linear(d_model, ntoken)# Capa de coneccion completa\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    # Iniciamos nuestros pesos de \"encoder\" con un rago de 0.1 ha -0.1\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        # Codificamos nuestros tonsor de datos\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        # Añadimos nuestra informacion posicional\n",
    "        src = self.pos_encoder(src)\n",
    "        # Pasamos nuestro Tensor de datos a nuestra red (Transformers)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        # Pasamos nuestro Tensor de datos a nuestras capa de coneccion completa\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Genera una matriz triangular superior de -inf, con ceros en diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egu-856ygg1G"
   },
   "source": [
    "El módulo ``PositionalEncoding`` inyecta información sobre el\n",
    "posición relativa o absoluta de las fichas en la secuencia.  los\n",
    "las codificaciones posicionales tienen la misma dimensión que las incrustaciones, de modo que\n",
    "los dos se pueden resumir.  Aquí, usamos las funciones ``seno`` y ``coseno`` de\n",
    "diferentes frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juHc2fwCgg1H"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Iniciamos anuestra capa de \"dropout\"\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        #Buscamos la posicion actual\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        #Busacamos la exponencial de los elementos para obtener el termino divisor\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        #Creamos informacion posicional binaria usando senos y cosenos\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Agregamos nuestra informacion posicional binaria\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        # Retornamos nuestra infromacion pasada por una capa de abandono\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26cFxqc2gg1I"
   },
   "source": [
    "Load and batch data\n",
    "-------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T49ZBSSlgg1J"
   },
   "source": [
    "Este tutorial usa ``torchtext`` para generar un conjunto de datos Wikitext-2.\n",
    " Para acceder a los conjuntos de datos de torchtext, instale torchdata siguiendo las instrucciones en https://github.com/pytorch/data.\n",
    "\n",
    "divide the alphabet into 4 sequences of length 6:\n",
    "\n",
    "El objeto de vocabulario se construye en base al conjunto de datos de entrenamiento y se usa para numerizar\n",
    "fichas en tensores. Wikitext-2 representa tokens raros como `<unk>`.\n",
    "\n",
    "Dado un vector 1-D de datos secuenciales, ``batchify()`` organiza los datos\n",
    "en columnas ``batch_size``. Si los datos no se dividen uniformemente en\n",
    "columnas ``batch_size``, luego los datos se recortan para que quepan. Por ejemplo, con\n",
    "el alfabeto como los datos (longitud total de 26) y ``batch_size=4``, lo haríamos\n",
    "dividir el alfabeto en 4 secuencias de longitud 6:\n",
    "\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "Batching enables more parallelizable processing. However, batching means that\n",
    "the model treats each column independently; for example, the dependence of\n",
    "``G`` and ``F`` can not be learned in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkyPAa_6m2jT"
   },
   "outputs": [],
   "source": [
    "#!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kTE2V_Xgg1K"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Dividimos nuestros datos de entrenamiento\n",
    "train_iter = WikiText2(split='train')\n",
    "\n",
    "# Creamos un tokenizador, ejemplo: https://pytorch.org/text/stable/data_utils.html\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    " \n",
    "# Creamos nuestro vocabulario usando como iterable nuestros datos \"train_iter\" tokenizados(con nuestra funcion \"map\")\n",
    "# y una lista de fichas especiales usamos el specials=['<unk>']\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>']) \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Convierte texto sin procesar en un tensor plano.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter fue \"consumido\" por el proceso de construcción del vocabulario,\n",
    "# así que tenemos que crearlo de nuevo\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divide los datos en secuencias bsz separadas, eliminando elementos adicionales\n",
    "    eso no encajaría limpiamente.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    # Divimos el primer elemnto de nuestro tensor entre el tamaño de nuestro minilote y lo redondeamos hacia abajo\n",
    "    seq_len = data.size(0) // bsz\n",
    "    # Divimos nuestros datos entre la logitud de nuestros mini lotes (seq_len * bsz)\n",
    "    data = data[:seq_len * bsz]\n",
    "    # Cambiamos la forma de nuestro Tensor para que:\n",
    "    # - Tenga la cantidad de columnas establecidas en \"bsz\"\n",
    "    # - Tenga la cantidad de filas establecidas en \"seq_len\"\n",
    "    # - El \".t()\" devuelven los tal cual tensores 0-D y 1-D \n",
    "    # - El \".contiguous()\" devuelve un tensor contiguo en memoria que contiene los mismos datos que selftensor\n",
    "    #   (Esto es nesesario si no sabe, si la nueva forma de tendor coincide con los datos que tiene)\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "\n",
    "    # Movemos nuestros datos a nuestro dispositivo (CPU/GPU)\n",
    "    return data.to(device)\n",
    "\n",
    "# Definimos el tamaño de nuestro mini lote de entrenamiento\n",
    "batch_size = 20\n",
    "# Definimos el tamaño de nuestro mini lote de evalucion \n",
    "eval_batch_size = 10\n",
    "\n",
    "# Creamos nuestros datos de entrenamiento\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "# Creamos nuestros datos de Evalucion\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "# Creamos nuestros datos de Testeo\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qimB_ZGygg1M"
   },
   "source": [
    "Funciones para generar entrada y secuencia de destino\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkdd0VXSgg1M"
   },
   "source": [
    "``get_batch()`` generates a pair of input-target sequences for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "\n",
    "\n",
    "``get_batch()`` genera un par de secuencias de entrada-objetivo para\n",
    "el modelo del transformador Subdivide los datos de origen en fragmentos de\n",
    "longitud ``bptt``. Para la tarea de modelado del lenguaje, el modelo necesita la\n",
    "siguientes palabras como ``Objetivo``. Por ejemplo, con un valor ``bptt`` de 2,\n",
    "obtendríamos las siguientes dos Variables para ``i`` = 0:\n",
    "\n",
    "Cabe señalar que los trozos están a lo largo de la dimensión 0, consistentes \n",
    "con la dimensión ``S`` en el modelo de Transformador.  La dimensión del lote\n",
    "``N`` está a lo largo de la dimensión 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBh9GGFvgg1N"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-9-exGmgg1N"
   },
   "source": [
    "Iniciar una instancia\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCu5Erzhgg1P"
   },
   "source": [
    "Los hiperparámetros del modelo se definen a continuación.  El tamaño del vocabulario es\n",
    "igual a la longitud del objeto de vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdXovHA7gg1P"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)# tamaño del vocabulario\n",
    "emsize = 200        # dimensión de incrustación/embedding \n",
    "d_hid = 200         # dimensión del modelo de red feedforward en nn.TransformerEncoder\n",
    "nlayers = 2         # número de nn.TransformerEncoderLayer en nn.TransformerEncoder\n",
    "nhead = 2           # número de cabezas en nn.MultiheadAttention\n",
    "dropout = 0.2       # probabilidad de abandono\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVnN7h9Fgg1R"
   },
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSwfuJXygg1R"
   },
   "source": [
    "Usamos `CrossEntropyLoss`con `SGD`(descenso de gradiente estocástico) optimizador. \n",
    "La tasa de aprendizaje se establece inicialmente en 5.0 y sigue un `StepLR`calendario. \n",
    "Durante el entrenamiento, usamos `nn.utils.clip_grad_norm\\`\n",
    "para evitar que los gradientes exploten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIGBetqbgg1S"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Criterio de evalucion\n",
    "lr = 5.0  # ratio de aprendizaje/learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)# Optimizador\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) #Decae la tasa de aprendizaje de cada grupo de parámetros por gamma cada épocas.\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # activar el modo de entrenamiento\n",
    "    total_loss = 0. # Iniciamos la perdida total\n",
    "    log_interval = 200 # Definimos la logitud de los intervalos\n",
    "    start_time = time.time()# iniciamos el tiempo actual\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)#Genera una matriz triangular y la mueve al dispositivo(CPU/GPU)\n",
    "\n",
    "    num_batches = len(train_data) // bptt # Definimos el numero de mini lotes\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)# Obtenemos nuestras Caracteristicas/Features y nuestro Objetivo/Target\n",
    "        batch_size = data.size(0)# Definimos el tamaño de nuestro mini lote\n",
    "        if batch_size != bptt:  # solo en el último lote\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)# Obtenemos la salida de nuestro modelo\n",
    "        loss = criterion(output.view(-1, ntokens), targets)# Calculamos la perdida del modelo\n",
    "\n",
    "        optimizer.zero_grad()# Vaciamos los gradientes\n",
    "        loss.backward()# Propagamos el error por la red\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)#Recorta la norma de gradiente de un iterable de parámetros.\n",
    "        optimizer.step()# Optimizamos nuestro modelo\n",
    "\n",
    "        total_loss += loss.item()# Sumamos nuestra perdida a la perdida total\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]# Obtenemos el ratio de aprendizaje/learning rate \n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval# Obtenemos el tiempo de ejecucion \n",
    "            cur_loss = total_loss / log_interval# Obtenemos la perdida \n",
    "            ppl = math.exp(cur_loss)# Calculamos la Perplejidad del modelo\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            # Reiniciamos la perdida y el tiempo\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # activar el modo de evaluación\n",
    "    total_loss = 0. # Iniciamos la perdida total\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)#Genera una matriz triangular y la mueve al dispositivo(CPU/GPU)\n",
    "    with torch.no_grad(): # Le decimos al modelo que NO aprenda de aqui\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i) # Obtenemos nuestras Caracteristicas/Features y nuestro Objetivo/Target\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:# solo en el último lote\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)# Obtenemos la salida/prediccion de nuestro modelo\n",
    "            output_flat = output.view(-1, ntokens)# Cambiamos la forma de nuestra salida/prediccion\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()# Sumamos la perdida de nuestro modelo en esta prediccion a la perdida total\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04qWTT0egg1T"
   },
   "source": [
    "Recorre las épocas.  Guarde el modelo si la pérdida de validación es la mejor\n",
    "hemos visto hasta ahora.  Ajuste la tasa de aprendizaje después de cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REIDkFRdgg1T",
    "outputId": "6f1dba67-1ed6-4d5b-ff34-a71ac51ee3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 26.43 | loss  8.13 | ppl  3384.21\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 14.01 | loss  6.86 | ppl   953.98\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 14.03 | loss  6.42 | ppl   613.99\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 14.05 | loss  6.29 | ppl   540.28\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 14.06 | loss  6.17 | ppl   479.99\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 14.08 | loss  6.15 | ppl   469.37\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 14.06 | loss  6.10 | ppl   447.05\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 14.07 | loss  6.10 | ppl   446.20\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 15.12 | loss  6.01 | ppl   409.20\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 14.13 | loss  6.01 | ppl   406.01\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 14.14 | loss  5.89 | ppl   359.71\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 14.92 | loss  5.97 | ppl   389.79\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 14.21 | loss  5.94 | ppl   380.94\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 14.22 | loss  5.86 | ppl   351.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 46.03s | valid loss  5.77 | valid ppl   320.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 14.35 | loss  5.85 | ppl   347.22\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 14.30 | loss  5.85 | ppl   345.67\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 14.32 | loss  5.65 | ppl   284.41\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 14.32 | loss  5.70 | ppl   297.44\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 14.34 | loss  5.64 | ppl   282.17\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 14.40 | loss  5.68 | ppl   292.35\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 14.47 | loss  5.69 | ppl   294.84\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.44 | loss  5.70 | ppl   299.74\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.51 | loss  5.64 | ppl   282.52\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.48 | loss  5.67 | ppl   289.16\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 14.53 | loss  5.54 | ppl   254.98\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.64 | ppl   280.62\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 14.56 | loss  5.64 | ppl   281.17\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.59 | loss  5.57 | ppl   261.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 44.21s | valid loss  5.63 | valid ppl   279.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 14.76 | loss  5.60 | ppl   270.40\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.72 | loss  5.61 | ppl   273.22\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.74 | loss  5.41 | ppl   223.85\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.77 | loss  5.48 | ppl   238.76\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.79 | loss  5.43 | ppl   227.60\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 14.85 | loss  5.47 | ppl   238.22\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 14.86 | loss  5.49 | ppl   241.46\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.85 | loss  5.51 | ppl   248.11\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 14.86 | loss  5.46 | ppl   234.81\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.88 | loss  5.48 | ppl   240.86\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 14.89 | loss  5.36 | ppl   211.78\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.86 | loss  5.46 | ppl   234.69\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.80 | loss  5.46 | ppl   236.27\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.78 | loss  5.40 | ppl   221.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 45.29s | valid loss  5.60 | valid ppl   270.34\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') # Definimos nuestro mejor valor de perdida\n",
    "epochs = 3 # Definimos el nuemero de epocas\n",
    "best_model = None # Definimos nuestro mejor modelo\n",
    "\n",
    "#Iteramos el modelo por cada epoca\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()# Contamos nuestro tiempo de inicio\n",
    "    train(model)#Entrenamos nuestro modelo\n",
    "    val_loss = evaluate(model, val_data)#Evaluamos nuestro modelo\n",
    "    val_ppl = math.exp(val_loss)# Camculamos la tasa de Perplejidad\n",
    "    elapsed = time.time() - epoch_start_time# Contamos nuestro tiempo transcurrido\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    #Si la perdida de nuestro modelo es mejor que el anterior\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss# Guardamos la nueva mejor perdida\n",
    "        best_model = copy.deepcopy(model)# Guardamos nuetro modelo \n",
    "\n",
    "    scheduler.step()# Modificamos nuestre ratio de aprendizaje/learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMtN-6MBgg1U"
   },
   "source": [
    "Evaluar el mejor modelo en el conjunto de datos de prueba\n",
    "-------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UerysqXjgg1U",
    "outputId": "d696e309-fd1d-4475-81e6-41e1627c73e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.51 | test ppl   247.52\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de transformer_tutorial.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
